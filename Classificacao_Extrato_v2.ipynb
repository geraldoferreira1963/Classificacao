{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geraldoferreira1963/Classificacao/blob/main/Classificacao_Extrato_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd1Ubn7qaToO",
        "outputId": "6bdb23ed-0c19-48af-8643-a28e64934628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "!pip install scikit-learn nltk gensim transformers -q\n",
        "!pip install requests -q\n",
        "!pip install scikeras -q\n",
        "!pip install joblib -q  #\n",
        "\n",
        "\n",
        "import joblib #\n",
        "import requests  #\n",
        "import tensorflow as tf  #\n",
        "import time\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.naive_bayes import GaussianNB #\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.utils import to_categorical #\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Model  #\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional #\n",
        "from google.colab import files\n",
        "from scikeras.wrappers import KerasClassifier  #\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgoEpN-oH9lw"
      },
      "source": [
        "# Objetivo : Análise do Dataframe com o dataset carregado, e gerar um primeiro conjunto de estatísticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zz71yRl0SG1T"
      },
      "outputs": [],
      "source": [
        "def analisar_dataframe(df):\n",
        "    # Contagem e Impressão do Conteúdo da Classificação\n",
        "    classificacao_contagem = df['Classificacao'].value_counts()\n",
        "    print(\"Distribuição da Classificacao:\")\n",
        "\n",
        "    # Contagem das classes de Classificacao\n",
        "    data = [['Classificacao', 'Contagem']]\n",
        "    for classificacao, contagem in classificacao_contagem.items():\n",
        "        data.append([classificacao, contagem])\n",
        "\n",
        "    # Formatação dos Dados\n",
        "    col_width = max(len(str(word)) for row in data for word in row) + 2  # padding\n",
        "    for row in data:\n",
        "        print(\"|\" + \"|\".join(str(word).ljust(col_width) for word in row) + \"|\")\n",
        "\n",
        "    # Calcular e Imprimir estatísticas\n",
        "\n",
        "    df['comprimento_conteudo'] = df['Operacao_Investimento'].apply(len)  #\n",
        "\n",
        "    # Imprimir Estatísticas do tamanho do conteúdo\n",
        "    print(\"\\n\\n Estatísticas do Comprimento do Conteúdo:\")\n",
        "    data = [\n",
        "        [\"Menor Comprimento\", df['comprimento_conteudo'].min()],\n",
        "        [\"Maior Comprimento\", df['comprimento_conteudo'].max()],\n",
        "        [\"Comprimento Mediano\", df['comprimento_conteudo'].median()]\n",
        "    ]\n",
        "\n",
        "    # Formatar Tabela\n",
        "    col_width = max(len(str(word)) for row in data for word in row) + 2  # padding\n",
        "    for row in data:\n",
        "        print(\"|\" + \"|\".join(str(word).ljust(col_width) for word in row) + \"|\")\n",
        "\n",
        "    comprimento_linha = df['comprimento_conteudo'].max()\n",
        "\n",
        "\n",
        "    # Cálculo do Tamanho do Vocabulário\n",
        "    all_words = ' '.join(df['Operacao_Investimento']).lower().split()\n",
        "    unique_words = set(all_words)\n",
        "    print(\"\\n\\n Número de Palavras:\", len(unique_words))\n",
        "\n",
        "    return(comprimento_linha,len(unique_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5XIsV8XAzzO"
      },
      "source": [
        "# Objetivo : Remoção de stop words, pontuações, opcionalmente lematização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GH3efbmfs-sx"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, lemmatize=True):\n",
        "\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Remoção de pontuações\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remoção de Stop Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7SiezA5fWp0"
      },
      "source": [
        "# Objetivo : Gerar diversos embeddings, o método será passado como parametro (TF-IDF, Word2Vec, BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4RPKMbQRfUv7"
      },
      "outputs": [],
      "source": [
        "def gerar_embedding(X_train, X_test, embedding_type='TF-IDF'):\n",
        "    if embedding_type == 'TF-IDF':\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_emb = vectorizer.fit_transform(X_train)\n",
        "        X_test_emb = vectorizer.transform(X_test)\n",
        "    elif embedding_type == 'Word2Vec':\n",
        "        sentences = [text.split() for text in X_train]\n",
        "        model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        X_train_emb = np.array([np.mean([model_w2v.wv[word] for word in text.split() if word in model_w2v.wv] or [np.zeros(100)], axis=0) for text in X_train])\n",
        "        X_test_emb = np.array([np.mean([model_w2v.wv[word] for word in text.split() if word in text.split() if word in model_w2v.wv] or [np.zeros(100)], axis=0) for text in X_test])\n",
        "    elif embedding_type == 'BERT':\n",
        "        tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "        model_bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "        def get_bert_embedding(text):\n",
        "            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "            outputs = model_bert(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
        "            # Reduce the number of features to 768 using PCA if necessary\n",
        "            if embeddings.shape[1] > 768:\n",
        "                from sklearn.decomposition import PCA\n",
        "                pca = PCA(n_components=768)\n",
        "                embeddings = pca.fit_transform(embeddings)\n",
        "            return embeddings\n",
        "\n",
        "        X_train_emb = np.array([get_bert_embedding(text) for text in X_train])\n",
        "        X_test_emb = np.array([get_bert_embedding(text) for text in X_test])\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Tipo de embedding não suportado.\")\n",
        "\n",
        "    return X_train_emb, X_test_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehR312N4JCPz"
      },
      "source": [
        "# Objetivo : Criar um modelo neural que utilizará como base uma camada CNN (número de filtros, e tamanho são parametros). Adicionalmente existem outros parametros (os quais serão Hiper Parametros) como : dropout e taxa de aprendizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-CzFapGJqHse"
      },
      "outputs": [],
      "source": [
        "def criacao_modelo_cnn(num_filters=32, filter_size=3, learning_rate=0.01, dropout_rate=0.2):\n",
        "    # Camada de Entrada ( entrada já estará vetorizada)\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "\n",
        "\n",
        "    # Camada Embedding, pesos seráo aprendidos durante o treinamento\n",
        "    embedding_layer = Embedding(input_dim=max_tokens, output_dim=128)(input_layer)\n",
        "\n",
        "    # Camada Convolutional\n",
        "    cnn_layer = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
        "\n",
        "    # Camada Max pooling\n",
        "    pooling_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
        "\n",
        "    # Flatten layer\n",
        "    flatten_layer = Flatten()(pooling_layer)\n",
        "\n",
        "    # Dropout layer\n",
        "    dropout_layer = Dropout(dropout_rate)(flatten_layer)\n",
        "\n",
        "    # Camada de Saída, Softmax, Classificação (Multi-Classe)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dropout_layer)\n",
        "\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKcphLZiKMFs"
      },
      "source": [
        "# Objetivo : Criar um modelo neural que utilizará como base uma camada LSTM  (número de neurons, bidirecionalidade são parametros). Adicionalmente existem outros parametros (os quais serão Hiper Parametros) como : dropout e taxa de aprendizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "E6pLzYHiqaah"
      },
      "outputs": [],
      "source": [
        "def criacao_modelo_lstm(num_neurons=64, bidirectional=True, learning_rate=0.01, dropout_rate=0.2):\n",
        "\n",
        "    # Camada de Entrada ( entrada já estará vetorizada)\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "\n",
        "\n",
        "    # Camada Embedding, pesos seráo aprendidos durante o treinamento\n",
        "    embedding_layer = Embedding(input_dim=max_tokens, output_dim=128)(input_layer)\n",
        "\n",
        "    # LSTM  (bidirectional ou unidirectional)\n",
        "    if bidirectional:\n",
        "        lstm_layer = Bidirectional(LSTM(num_neurons))(embedding_layer)\n",
        "    else:\n",
        "        lstm_layer = LSTM(num_neurons)(embedding_layer)\n",
        "\n",
        "    # Dropout layer\n",
        "    dropout_layer = Dropout(dropout_rate)(lstm_layer)\n",
        "\n",
        "    # Camada de Saída com softmax ( multi-class classification )\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dropout_layer)\n",
        "\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqZcBohZQ2VS"
      },
      "source": [
        "# Objetivo : Imprimir Resultados dos Treinamentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tyeBsdIjgM54"
      },
      "outputs": [],
      "source": [
        "def imprimir_resultados_tabela(results):\n",
        "    \"\"\"Prints the results in a formatted table.\"\"\"\n",
        "    header = [\"Modelo\", \"Embedding\", \"Acurácia\"]\n",
        "    col_widths = [max(len(str(x)) for x in col) for col in zip(*([header] + results))]  #\n",
        "\n",
        "    # Cabeçario\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
        "    print(\"|\" + \"|\".join(str(x).center(width + 2) for x, width in zip(header, col_widths)) + \"|\")\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
        "\n",
        "    # Linhas\n",
        "    for row in results:\n",
        "        print(\"|\" + \"|\".join(str(x).center(width + 2) for x, width in zip(row, col_widths)) + \"|\")\n",
        "\n",
        "    #\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5jY1rLBBI-3M"
      },
      "outputs": [],
      "source": [
        "def imprimir_resultados_tabela_II (results):\n",
        "    \"\"\"Prints the results in a formatted table.\"\"\"\n",
        "    header = [\"Modelo\", \"Acuracia\", \"Acurácia\"]\n",
        "    col_widths = [max(len(str(x)) for x in col) for col in zip(*([header] + results))]  #\n",
        "\n",
        "    # Cabeçario\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
        "    print(\"|\" + \"|\".join(str(x).center(width + 2) for x, width in zip(header, col_widths)) + \"|\")\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
        "\n",
        "    # Linhas\n",
        "    for row in results:\n",
        "        print(\"|\" + \"|\".join(str(x).center(width + 2) for x, width in zip(row, col_widths)) + \"|\")\n",
        "\n",
        "    #\n",
        "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "m58XOTQWC-Hv"
      },
      "outputs": [],
      "source": [
        "def carregar_modelo_do_github(model_name, embedding_name):\n",
        "    filename = f\"{model_name}_{embedding_name}_best_model.pkl\"\n",
        "    github_url = f\"https://raw.githubusercontent.com/geraldoferreira1963/Classificacao/main/{filename}\"  #  GitHub URL\n",
        "\n",
        "    # Verifica se o GitHub\n",
        "    response = requests.get(github_url)\n",
        "    if response.status_code == 200:\n",
        "        # File existe no GitHub download e carrega\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        model = joblib.load(filename)\n",
        "        print(f\"Carregando modelo  {model_name} do GitHub.\")\n",
        "    else:\n",
        "        # Arquivo não esta no GitHub, mensagem erro\n",
        "        raise FileNotFoundError(f\"Arquivo {filename} não encontrado no GitHub.\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjv8OTMwQ7vl"
      },
      "source": [
        "# Carga do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AVG8SM59W4_Z",
        "outputId": "4ca25885-722b-4caf-9ab4-ec4b84854c74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Operacao_Investimento    Classificacao\n",
              "0   Câmbio Padrão : R$5977.29  Operacao_Outras\n",
              "1  Câmbio Padrão : R$10460.25  Operacao_Outras\n",
              "2  Câmbio Padrão : R$10460.25  Operacao_Outras\n",
              "3   Câmbio Padrão : R$9962.14  Operacao_Outras\n",
              "4   Compra de 60 LTC a $ 43.3   Operacao_Bolsa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fb7c373-d5b0-4696-81a5-d6ed7e33740e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Operacao_Investimento</th>\n",
              "      <th>Classificacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Câmbio Padrão : R$5977.29</td>\n",
              "      <td>Operacao_Outras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Câmbio Padrão : R$10460.25</td>\n",
              "      <td>Operacao_Outras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Câmbio Padrão : R$10460.25</td>\n",
              "      <td>Operacao_Outras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Câmbio Padrão : R$9962.14</td>\n",
              "      <td>Operacao_Outras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Compra de 60 LTC a $ 43.3</td>\n",
              "      <td>Operacao_Bolsa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fb7c373-d5b0-4696-81a5-d6ed7e33740e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0fb7c373-d5b0-4696-81a5-d6ed7e33740e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0fb7c373-d5b0-4696-81a5-d6ed7e33740e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2fc5f136-271b-4a16-b416-37d61fd7c9a9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fc5f136-271b-4a16-b416-37d61fd7c9a9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2fc5f136-271b-4a16-b416-37d61fd7c9a9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3343,\n  \"fields\": [\n    {\n      \"column\": \"Operacao_Investimento\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1059,\n        \"samples\": [\n          \"RECEBIMENTO DE TED - SPB\",\n          \"Cr\\u00e9dito de cupom (CVS 5.05 25/03/48)\",\n          \"Compra de 10 PCEF a $ 23.7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Classificacao\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rendimento_RF\",\n          \"Operacao_Bolsa\",\n          \"Taxa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "\n",
        "url = 'https://raw.githubusercontent.com/geraldoferreira1963/Analise_Sentimento/main/carga_inicial_4.xlsx'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "with open('carga_inicial_4.xlsx', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "df = pd.read_excel('carga_inicial_4.xlsx', engine='openpyxl') #\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNNvabhRJFK"
      },
      "source": [
        "# Análise Inicial do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0_qDj6cXSfR",
        "outputId": "30998bb6-af75-452e-c544-24af2a22ea48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuição da Classificacao:\n",
            "|Classificacao    |Contagem         |\n",
            "|Imposto          |870              |\n",
            "|Rendimento_Acao  |542              |\n",
            "|Rendimento_FII   |462              |\n",
            "|Operacao_Bolsa   |351              |\n",
            "|Operacao_Outras  |314              |\n",
            "|Aluguel          |245              |\n",
            "|Taxa             |209              |\n",
            "|Resgate_RF       |160              |\n",
            "|Operacao_RF      |142              |\n",
            "|Rendimento_RF    |48               |\n",
            "\n",
            "\n",
            " Estatísticas do Comprimento do Conteúdo:\n",
            "|Menor Comprimento    |12                   |\n",
            "|Maior Comprimento    |80                   |\n",
            "|Comprimento Mediano  |41.0                 |\n",
            "\n",
            "\n",
            " Número de Palavras: 1291\n"
          ]
        }
      ],
      "source": [
        "# Verificação Inicial do dataset\n",
        "max_length, unique_words = analisar_dataframe(df)  #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjQDNJiiRT3k"
      },
      "source": [
        "# Modelos Não Neurais - Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "EKW_y8iQaESj"
      },
      "outputs": [],
      "source": [
        "# Preprocessamento da coluna Operação Investimento remoção de pontuação, stop words\n",
        "df['Operacao_Investimento'] = df['Operacao_Investimento'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Divisão dos dados\n",
        "X = df['Operacao_Investimento'].apply(preprocess_text)\n",
        "y = df['Classificacao']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Modelos e Hiper Parametros\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SVM': SVC()\n",
        "}\n",
        "\n",
        "# Hiper Parametros\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'C': [0.1, 1, 10]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'C': [0.1, 1, 10]\n",
        "    }\n",
        "}\n",
        "\n",
        "embeddings = {\n",
        "    'TF-IDF': (X_train_tfidf, X_test_tfidf),\n",
        "    'Word2Vec': (X_train_w2v, X_test_w2v),\n",
        "    'BERT': (X_train_bert.squeeze(1), X_test_bert.squeeze(1))\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne6baLxg1UWZ"
      },
      "source": [
        "# Modelos Não Neurais - Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "0I-CNAkoApSd",
        "outputId": "2548aaed-504d-4449-ebed-07bb2d339cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiper Parametros: {'C': 10, 'solver': 'saga'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9718be8b-6d0c-467b-a576-5556ffbbee0e\", \"Logistic Regression_TF-IDF_best_model.pkl\", 66167)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo Logistic Regression com  TF-IDF salvo no  Logistic Regression_TF-IDF_best_model.pkl\n",
            "Tempo de Treinamento do modelo Logistic Regression (TF-IDF): 3.0362 segundos\n",
            "Melhores Hiper Parametros: {'C': 10, 'solver': 'saga'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7bd1053-2172-4015-803a-9eb4df83946e\", \"Logistic Regression_Word2Vec_best_model.pkl\", 5247)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo Logistic Regression com  Word2Vec salvo no  Logistic Regression_Word2Vec_best_model.pkl\n",
            "Tempo de Treinamento do modelo Logistic Regression (Word2Vec): 19.7637 segundos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiper Parametros: {'C': 10, 'solver': 'liblinear'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1b9bcb7a-5f09-4d54-9ab7-551b2a6b0566\", \"Logistic Regression_BERT_best_model.pkl\", 62767)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo Logistic Regression com  BERT salvo no  Logistic Regression_BERT_best_model.pkl\n",
            "Tempo de Treinamento do modelo Logistic Regression (BERT): 835.8428 segundos\n",
            "Melhores Hiper Parametros: {'C': 10, 'kernel': 'linear'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68ce3ad1-7585-4df0-91c7-b85b7c1603ca\", \"SVM_TF-IDF_best_model.pkl\", 170123)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo SVM com  TF-IDF salvo no  SVM_TF-IDF_best_model.pkl\n",
            "Tempo de Treinamento do modelo SVM (TF-IDF): 6.0570 segundos\n",
            "Melhores Hiper Parametros: {'C': 10, 'kernel': 'rbf'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fc88f837-a8ab-430d-95fd-5b6f7f91c1ec\", \"SVM_Word2Vec_best_model.pkl\", 1652667)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo SVM com  Word2Vec salvo no  SVM_Word2Vec_best_model.pkl\n",
            "Tempo de Treinamento do modelo SVM (Word2Vec): 11.6047 segundos\n",
            "Melhores Hiper Parametros: {'C': 1, 'kernel': 'linear'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6d7450b9-8a1d-4190-b539-b4e0e3c739a1\", \"SVM_BERT_best_model.pkl\", 2249387)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo SVM com  BERT salvo no  SVM_BERT_best_model.pkl\n",
            "Tempo de Treinamento do modelo SVM (BERT): 16.2293 segundos\n"
          ]
        }
      ],
      "source": [
        "# Geração dos Embeddings\n",
        "X_train_tfidf, X_test_tfidf = gerar_embedding(X_train, X_test, embedding_type='TF-IDF')\n",
        "X_train_w2v, X_test_w2v = gerar_embedding(X_train, X_test, embedding_type='Word2Vec')\n",
        "X_train_bert, X_test_bert = gerar_embedding(X_train, X_test, embedding_type='BERT')\n",
        "\n",
        "trained_models = {}\n",
        "for model_name, model in models.items():\n",
        "    for embedding_name, (X_train_emb, X_test_emb) in embeddings.items():\n",
        "        start_time = time.time()  # Iniciando contagem do tempo\n",
        "\n",
        "        if model_name in param_grids:\n",
        "            grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy')\n",
        "            grid_search.fit(X_train_emb, y_train)\n",
        "            model = grid_search.best_estimator_\n",
        "            print(f\"Melhores Hiper Parametros: {grid_search.best_params_}\")\n",
        "\n",
        "            # Salvando o o melhor modelo e seus hiper parametros\n",
        "            filename = f\"{model_name}_{embedding_name}_best_model.pkl\"\n",
        "            joblib.dump(model, filename)\n",
        "            files.download(filename)\n",
        "            print(f\"Melhor modelo {model_name} com  {embedding_name} salvo no  {filename}\")\n",
        "\n",
        "\n",
        "        if model_name == 'Naive Bayes' and embedding_name in ['TF-IDF']:\n",
        "            X_train_emb = X_train_emb.toarray()\n",
        "            X_test_emb = X_test_emb.toarray()\n",
        "\n",
        "        #\n",
        "        model.fit(X_train_emb, y_train)  # Treinamento\n",
        "\n",
        "        end_time = time.time()  # Fim da contagem do tempo\n",
        "        training_time = end_time - start_time  # Calcular tempo\n",
        "\n",
        "        print(f\"Tempo de Treinamento do modelo {model_name} ({embedding_name}): {training_time:.4f} segundos\")\n",
        "\n",
        "        # Armazenar o Modelo\n",
        "        trained_models[(model_name, embedding_name)] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQovSTdl1cUx"
      },
      "source": [
        "# Modelos Não Neurais - Avaliação e Impressão dos Resultados"
      ]
    },
    {
      "source": [
        "results = []\n",
        "\n",
        "for (model_name, embedding_name), model in trained_models.items():\n",
        "\n",
        "    print(f\"Usando o modelo treinado {model_name} com {embedding_name}.\")\n",
        "\n",
        "    X_test_emb = embeddings[embedding_name][1]\n",
        "    y_pred = model.predict(X_test_emb)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Acurácia para o modelo de {model_name} ({embedding_name}): {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    results.append([model_name, embedding_name, accuracy])\n",
        "\n",
        "# Impressão dos Resultados\n",
        "imprimir_resultados_tabela(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYOF9QMetudO",
        "outputId": "075d2d0a-ec51-4494-8f49-e6ad84bfc878"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando o modelo treinado Logistic Regression com TF-IDF.\n",
            "Acurácia para o modelo de Logistic Regression (TF-IDF): 0.9895\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       0.99      1.00      1.00       167\n",
            " Operacao_Bolsa       0.97      1.00      0.99        71\n",
            "Operacao_Outras       1.00      0.98      0.99        61\n",
            "    Operacao_RF       1.00      0.92      0.96        25\n",
            "Rendimento_Acao       0.97      1.00      0.98       115\n",
            " Rendimento_FII       1.00      0.99      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       1.00      0.97      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.99      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Usando o modelo treinado Logistic Regression com Word2Vec.\n",
            "Acurácia para o modelo de Logistic Regression (Word2Vec): 0.7414\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      0.87      0.93        53\n",
            "        Imposto       0.70      0.95      0.81       167\n",
            " Operacao_Bolsa       0.68      0.63      0.66        71\n",
            "Operacao_Outras       1.00      0.57      0.73        61\n",
            "    Operacao_RF       0.83      0.80      0.82        25\n",
            "Rendimento_Acao       0.60      0.64      0.62       115\n",
            " Rendimento_FII       0.77      0.95      0.85        86\n",
            "  Rendimento_RF       0.00      0.00      0.00        10\n",
            "     Resgate_RF       1.00      0.81      0.89        31\n",
            "           Taxa       0.67      0.20      0.31        50\n",
            "\n",
            "       accuracy                           0.74       669\n",
            "      macro avg       0.73      0.64      0.66       669\n",
            "   weighted avg       0.75      0.74      0.72       669\n",
            "\n",
            "Usando o modelo treinado Logistic Regression com BERT.\n",
            "Acurácia para o modelo de Logistic Regression (BERT): 0.9851\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       0.99      0.97      0.98        71\n",
            "Operacao_Outras       1.00      0.95      0.97        61\n",
            "    Operacao_RF       0.92      0.96      0.94        25\n",
            "Rendimento_Acao       0.97      0.99      0.98       115\n",
            " Rendimento_FII       0.98      0.99      0.98        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       0.97      1.00      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.98      0.97      0.97       669\n",
            "   weighted avg       0.99      0.99      0.98       669\n",
            "\n",
            "Usando o modelo treinado SVM com TF-IDF.\n",
            "Acurácia para o modelo de SVM (TF-IDF): 0.9910\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       0.99      1.00      1.00       167\n",
            " Operacao_Bolsa       0.99      1.00      0.99        71\n",
            "Operacao_Outras       1.00      0.98      0.99        61\n",
            "    Operacao_RF       1.00      0.96      0.98        25\n",
            "Rendimento_Acao       0.97      1.00      0.98       115\n",
            " Rendimento_FII       1.00      0.99      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       1.00      0.97      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.99      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Usando o modelo treinado SVM com Word2Vec.\n",
            "Acurácia para o modelo de SVM (Word2Vec): 0.7952\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      0.87      0.93        53\n",
            "        Imposto       0.71      0.96      0.82       167\n",
            " Operacao_Bolsa       0.70      0.80      0.75        71\n",
            "Operacao_Outras       1.00      0.59      0.74        61\n",
            "    Operacao_RF       0.85      0.88      0.86        25\n",
            "Rendimento_Acao       0.72      0.83      0.77       115\n",
            " Rendimento_FII       0.98      0.94      0.96        86\n",
            "  Rendimento_RF       0.00      0.00      0.00        10\n",
            "     Resgate_RF       0.89      0.81      0.85        31\n",
            "           Taxa       0.82      0.18      0.30        50\n",
            "\n",
            "       accuracy                           0.80       669\n",
            "      macro avg       0.77      0.69      0.70       669\n",
            "   weighted avg       0.81      0.80      0.77       669\n",
            "\n",
            "Usando o modelo treinado SVM com BERT.\n",
            "Acurácia para o modelo de SVM (BERT): 0.9880\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       1.00      0.97      0.99        71\n",
            "Operacao_Outras       1.00      0.95      0.97        61\n",
            "    Operacao_RF       0.93      1.00      0.96        25\n",
            "Rendimento_Acao       0.98      0.99      0.99       115\n",
            " Rendimento_FII       0.98      1.00      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       0.94      1.00      0.97        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.98      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "+---------------------+-----------+--------------------+\n",
            "|        Modelo       | Embedding |      Acurácia      |\n",
            "+---------------------+-----------+--------------------+\n",
            "| Logistic Regression |   TF-IDF  | 0.9895366218236173 |\n",
            "| Logistic Regression |  Word2Vec | 0.7414050822122571 |\n",
            "| Logistic Regression |    BERT   | 0.9850523168908819 |\n",
            "|         SVM         |   TF-IDF  | 0.9910313901345291 |\n",
            "|         SVM         |  Word2Vec | 0.7952167414050823 |\n",
            "|         SVM         |    BERT   | 0.9880418535127056 |\n",
            "+---------------------+-----------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos Não Neurais - Predições"
      ],
      "metadata": {
        "id": "jwgu67XJWRsj"
      }
    },
    {
      "source": [
        "model_files = [\n",
        "    \"Logistic Regression_TF-IDF_best_model.pkl\",\n",
        "    \"SVM_BERT_best_model.pkl\",\n",
        "    \"SVM_TF-IDF_best_model.pkl\",\n",
        "    \"SVM_Word2Vec_best_model.pkl\",\n",
        "    \"Logistic Regression_BERT_best_model.pkl\",\n",
        "    \"Logistic Regression_Word2Vec_best_model.pkl\"\n",
        "]\n",
        "\n",
        "for model_file in model_files:\n",
        "    parts = model_file.split(\"_\")\n",
        "    model_name = parts[0]  # e.g., \"Logistic Regression\"\n",
        "    if parts[1] != \"best\":\n",
        "        embedding_name = parts[1]  # e.g., \"TF-IDF\"\n",
        "    else:\n",
        "        embedding_name = parts[0]\n",
        "\n",
        "    try:\n",
        "        # Carregar o modelo usando carregar_modelo_do_github:\n",
        "        model = carregar_modelo_do_github(model_name, embedding_name)\n",
        "        print(f\"Carregando modelo {model_name} do GitHub.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error na carga do modelo {model_file} do GitHub.\")\n",
        "        continue\n",
        "\n",
        "    X_test_emb = embeddings[embedding_name][1]\n",
        "\n",
        "    # 4. Predição:\n",
        "    y_pred = model.predict(X_test_emb)\n",
        "\n",
        "    # 5. Avaliação:\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Acurácia do modelo {model_name} ({embedding_name}): {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPx_PFo2v7nQ",
        "outputId": "e167204d-c55c-4382-b70a-355c45aa2257"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando modelo  Logistic Regression do GitHub.\n",
            "Carregando modelo Logistic Regression do GitHub.\n",
            "Acurácia do modelo Logistic Regression (TF-IDF): 0.9895\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       0.99      1.00      1.00       167\n",
            " Operacao_Bolsa       0.97      1.00      0.99        71\n",
            "Operacao_Outras       1.00      0.98      0.99        61\n",
            "    Operacao_RF       1.00      0.92      0.96        25\n",
            "Rendimento_Acao       0.97      1.00      0.98       115\n",
            " Rendimento_FII       1.00      0.99      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       1.00      0.97      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.99      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Carregando modelo  SVM do GitHub.\n",
            "Carregando modelo SVM do GitHub.\n",
            "Acurácia do modelo SVM (BERT): 0.9880\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       1.00      0.97      0.99        71\n",
            "Operacao_Outras       1.00      0.95      0.97        61\n",
            "    Operacao_RF       0.93      1.00      0.96        25\n",
            "Rendimento_Acao       0.98      0.99      0.99       115\n",
            " Rendimento_FII       0.98      1.00      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       0.94      1.00      0.97        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.98      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Carregando modelo  SVM do GitHub.\n",
            "Carregando modelo SVM do GitHub.\n",
            "Acurácia do modelo SVM (TF-IDF): 0.9910\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       0.99      1.00      1.00       167\n",
            " Operacao_Bolsa       0.99      1.00      0.99        71\n",
            "Operacao_Outras       1.00      0.98      0.99        61\n",
            "    Operacao_RF       1.00      0.96      0.98        25\n",
            "Rendimento_Acao       0.97      1.00      0.98       115\n",
            " Rendimento_FII       1.00      0.99      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       1.00      0.97      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.99      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Carregando modelo  SVM do GitHub.\n",
            "Carregando modelo SVM do GitHub.\n",
            "Acurácia do modelo SVM (Word2Vec): 0.7952\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      0.87      0.93        53\n",
            "        Imposto       0.71      0.96      0.82       167\n",
            " Operacao_Bolsa       0.70      0.80      0.75        71\n",
            "Operacao_Outras       1.00      0.59      0.74        61\n",
            "    Operacao_RF       0.85      0.88      0.86        25\n",
            "Rendimento_Acao       0.72      0.83      0.77       115\n",
            " Rendimento_FII       0.98      0.94      0.96        86\n",
            "  Rendimento_RF       0.00      0.00      0.00        10\n",
            "     Resgate_RF       0.89      0.81      0.85        31\n",
            "           Taxa       0.82      0.18      0.30        50\n",
            "\n",
            "       accuracy                           0.80       669\n",
            "      macro avg       0.77      0.69      0.70       669\n",
            "   weighted avg       0.81      0.80      0.77       669\n",
            "\n",
            "Carregando modelo  Logistic Regression do GitHub.\n",
            "Carregando modelo Logistic Regression do GitHub.\n",
            "Acurácia do modelo Logistic Regression (BERT): 0.9851\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       0.99      0.97      0.98        71\n",
            "Operacao_Outras       1.00      0.95      0.97        61\n",
            "    Operacao_RF       0.92      0.96      0.94        25\n",
            "Rendimento_Acao       0.97      0.99      0.98       115\n",
            " Rendimento_FII       0.98      0.99      0.98        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       0.97      1.00      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.98      0.97      0.97       669\n",
            "   weighted avg       0.99      0.99      0.98       669\n",
            "\n",
            "Carregando modelo  Logistic Regression do GitHub.\n",
            "Carregando modelo Logistic Regression do GitHub.\n",
            "Acurácia do modelo Logistic Regression (Word2Vec): 0.7414\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      0.87      0.93        53\n",
            "        Imposto       0.70      0.95      0.81       167\n",
            " Operacao_Bolsa       0.68      0.63      0.66        71\n",
            "Operacao_Outras       1.00      0.57      0.73        61\n",
            "    Operacao_RF       0.83      0.80      0.82        25\n",
            "Rendimento_Acao       0.60      0.64      0.62       115\n",
            " Rendimento_FII       0.77      0.95      0.85        86\n",
            "  Rendimento_RF       0.00      0.00      0.00        10\n",
            "     Resgate_RF       1.00      0.81      0.89        31\n",
            "           Taxa       0.67      0.20      0.31        50\n",
            "\n",
            "       accuracy                           0.74       669\n",
            "      macro avg       0.73      0.64      0.66       669\n",
            "   weighted avg       0.75      0.74      0.72       669\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX7nUVjL0LCY"
      },
      "source": [
        "# Modelos Neurais - Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDD8kw8Y0Gj1"
      },
      "outputs": [],
      "source": [
        "df['Operacao_Investimento'] = df['Operacao_Investimento'].apply(preprocess_text)\n",
        "\n",
        "#\n",
        "all_classes = df['Classificacao'].unique()\n",
        "\n",
        "# Gerando Encoders\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_classes)\n",
        "\n",
        "# Divisão dos Dados\n",
        "X = df['Operacao_Investimento']\n",
        "y = df['Classificacao']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vetorização da Operação Investimento\n",
        "max_tokens = unique_words\n",
        "max_length = max_length\n",
        "vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=max_length)\n",
        "vectorizer.adapt(X_train)\n",
        "\n",
        "\n",
        "# Transformando os dados de treinamento e teste em vetores\n",
        "X_train_vec = vectorizer(X_train).numpy()\n",
        "X_test_vec = vectorizer(X_test).numpy()\n",
        "\n",
        "# Transformandos os dados de Label (Classificação ) em representações númericas\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Realizando o one-hot encoding de Classificação\n",
        "y_train_encoded = to_categorical(y_train_encoded)\n",
        "y_test_encoded = to_categorical(y_test_encoded)\n",
        "\n",
        "\n",
        "num_classes = y_train_encoded.shape[1]\n",
        "\n",
        "# Models e  Hiper Parametros\n",
        "\n",
        "models = {\n",
        "    'CNN': {\n",
        "        'create_model': criacao_modelo_cnn,\n",
        "        'param_grid': {\n",
        "          #  'model__num_filters': [32, 64], # comentado decorrente da falta de memória para considerar este parametro no espaço de busca\n",
        "         #   'model__filter_size': [3, 5],   # comentado decorrente da falta de memória para considerar este parametro no espaço de busca\n",
        "            'model__learning_rate': [0.01, 0.001],\n",
        "            'model__dropout_rate': [0.2, 0.5],\n",
        "            'batch_size': [32, 64],\n",
        "            'epochs': [10, 20]\n",
        "        }\n",
        "    },\n",
        "    'LSTM': {\n",
        "        'create_model': criacao_modelo_lstm,\n",
        "        'param_grid': {\n",
        "            'model__num_neurons': [32, 64],\n",
        "           # 'model__bidirectional': [True, False],\n",
        "            'model__learning_rate': [0.01, 0.001],\n",
        "            'model__dropout_rate': [0.2, 0.5],\n",
        "            'batch_size': [32, 64],\n",
        "            'epochs': [10, 20]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD2ahXdu_XTm"
      },
      "source": [
        "# Modelos Neurais - Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m58gUOH_-K1T",
        "outputId": "162bbfd4-0aff-4bdb-caf3-6db6734d3563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinamento do modelo  CNN ...\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Melhores Hiper parametros para o  CNN: {'batch_size': 32, 'epochs': 10, 'model__dropout_rate': 0.2, 'model__learning_rate': 0.001}\n",
            "Treinamento do modelo  LSTM ...\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Melhores Hiper parametros para o  LSTM: {'batch_size': 64, 'epochs': 20, 'model__dropout_rate': 0.2, 'model__learning_rate': 0.01, 'model__num_neurons': 64}\n",
            "Melhor modelo CNN salvo em CNN_best_model.keras\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0ee0b70-de56-4af1-92fd-0b5f63454ac9\", \"CNN_best_model.keras\", 2310234)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor modelo LSTM salvo em LSTM_best_model.keras\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40ba7c63-30fc-4b59-92ed-e1216dce3a23\", \"LSTM_best_model.keras\", 3222738)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "best_models = {}\n",
        "\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"Treinamento do modelo  {model_name} ...\")\n",
        "    best_accuracy = 0.0  # Inicializando\n",
        "\n",
        "    # Iniciando a busca os melhores hiper parametros\n",
        "    model = KerasClassifier(model=model_info['create_model'])\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=model_info['param_grid'], cv=5, scoring='accuracy',verbose =0)\n",
        "    grid_result = grid_search.fit(X_train_vec, y_train_encoded, verbose =0)\n",
        "\n",
        "    # Imprimeir os melhores hiper parametros\n",
        "    print(f\"Melhores Hiper parametros para o  {model_name}: {grid_result.best_params_}\")\n",
        "\n",
        "    # Atualizar melhor acurácia e melhor modelo\n",
        "    current_accuracy = grid_result.best_score_\n",
        "    if current_accuracy > best_accuracy:\n",
        "        best_accuracy = current_accuracy\n",
        "        best_models[model_name] = grid_result.best_estimator_  # Store the best KerasClassifier\n",
        "\n",
        "# 2. Gerando os arquivos com os melhores modelos (CNN e LSTM)\n",
        "for model_name, best_model in best_models.items():\n",
        "    filename = f\"{model_name}_best_model.keras\"\n",
        "    best_model.model_.save(filename)  # Salvando o modelo\n",
        "    print(f\"Melhor modelo {model_name} salvo em {filename}\")\n",
        "    files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_tLeuvX_3Mq"
      },
      "source": [
        "# Modelos Neurais - Avaliação e Impressão dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Nj-7ER3_-cnX",
        "outputId": "138ffe6a-cbd3-46c2-d303-a3e4664e77df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-de5d7ac23a68>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Lista para armazenar os resultados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{model_name}_best_model.keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_models' is not defined"
          ]
        }
      ],
      "source": [
        "results = []  # Lista para armazenar os resultados\n",
        "\n",
        "for model_name, best_model in best_models.items():\n",
        "    filename = f\"{model_name}_best_model.keras\"\n",
        "\n",
        "    # Carga dos melhores modelos (ou do arquivo, ou de best model)\n",
        "    #trained_keras_model = tf.keras.models.load_model(filename)\n",
        "\n",
        "    filename = f\"{model_name}_best_model.keras\"\n",
        "    github_url = f\"https://raw.githubusercontent.com/geraldoferreira1963/Classificacao/main/{filename}\"  #  GitHub URL\n",
        "\n",
        "    # Check if file exists on GitHub\n",
        "    response = requests.get(github_url)\n",
        "    if response.status_code == 200:\n",
        "        # Arquivo esta no  GitHub, download e carrega\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        trained_keras_model = tf.keras.models.load_model(filename)\n",
        "        print(f\"Carregando modelo {model_name} do GitHub.\")\n",
        "    else:\n",
        "        # Arquivo não esta no  GitHub, best_model\n",
        "        trained_keras_model = best_model.model_  # Use o modelo do best model\n",
        "        print(f\"Usando melhor modelo {model_name} do tunning dos hiper parametros.\")\n",
        "\n",
        "\n",
        "    # Avaliar o modelo\n",
        "    _, accuracy = trained_keras_model.evaluate(X_test_vec, y_test_encoded, verbose=1)\n",
        "    print(f\"Acurácia para o  {model_name}: {accuracy:.4f}\")\n",
        "\n",
        "    # Realizando a Predição\n",
        "    y_pred_probs = trained_keras_model.predict(X_test_vec)  #\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)      #\n",
        "    y_pred = label_encoder.inverse_transform(y_pred) #\n",
        "\n",
        "    # Gerar e Imprimir a Classificação do Report\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Atualizando resultados\n",
        "    results.append([model_name, accuracy])\n",
        "\n",
        "#Impressão e Formatação dos Resultados\n",
        "imprimir_resultados_tabela_II(results)"
      ]
    },
    {
      "source": [
        "for model_info in model_files:\n",
        "\n",
        "    parts = model_info.split(\"_\")\n",
        "    model_name = parts[0]\n",
        "    if parts[1] != \"best\":\n",
        "        embedding_name = parts[1]\n",
        "    else:\n",
        "        embedding_name = parts[0]\n",
        "\n",
        "    # Obtendo modelos do best_models\n",
        "    model = best_models.get((model_name, embedding_name))\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"Modelo {model_name} com {embedding_name} não encontrado em best_models.\")\n",
        "        continue  # Skip to the next model if not found\n",
        "\n",
        "    # dados embedding\n",
        "    X_test_emb = embeddings[embedding_name][1]\n",
        "\n",
        "    # 4. Predição:\n",
        "    y_pred = model.predict(X_test_emb)\n",
        "\n",
        "    # 5. Avaliação:\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Acurácia para o modelo {model_name} ({embedding_name}): {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Jzp-NFgqw_Nv",
        "outputId": "e4ee48af-e2b4-44a1-ee25-8520ae825570"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4f125beb5c76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Obtendo modelos do best_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos Neurais - Predições"
      ],
      "metadata": {
        "id": "fy2gGXI_Z_Qv"
      }
    },
    {
      "source": [
        "model_files = [\n",
        "    \"LSTM_best_model.keras\",\n",
        "    \"CNN_best_model.keras\"\n",
        "]\n",
        "\n",
        "for model_file in model_files:\n",
        "\n",
        "    model_name = model_file.split(\"_\")[0]  # e.g., \"LSTM\"\n",
        "\n",
        "    # 2. Carga do modelo  GitHub:\n",
        "    github_url = f\"https://raw.githubusercontent.com/geraldoferreira1963/Classificacao/main/{model_file}\"\n",
        "    response = requests.get(github_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(model_file, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        if os.path.exists(model_file):\n",
        "            model = tf.keras.models.load_model(model_file)\n",
        "            print(f\"Modelo  {model_name} carregado do GitHub.\")\n",
        "        else:\n",
        "            print(f\"Error: {model_file} não encontrado.\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"Error na carga do {model_file} do GitHub.\")\n",
        "        continue\n",
        "\n",
        "    # 3. Predição\n",
        "    y_pred_probs = model.predict(X_test_vec)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "    # 4. Avaliação :\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1OihtrqZImA",
        "outputId": "c8e26aa6-3615-4045-e7d2-fd498fcd09bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded LSTM model from GitHub.\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n",
            "Accuracy for LSTM: 0.9910\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       1.00      1.00      1.00        71\n",
            "Operacao_Outras       1.00      0.97      0.98        61\n",
            "    Operacao_RF       1.00      0.96      0.98        25\n",
            "Rendimento_Acao       0.98      0.99      0.99       115\n",
            " Rendimento_FII       0.98      1.00      0.99        86\n",
            "  Rendimento_RF       0.89      0.80      0.84        10\n",
            "     Resgate_RF       0.97      1.00      0.98        31\n",
            "           Taxa       1.00      1.00      1.00        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.98      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n",
            "Loaded CNN model from GitHub.\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "Accuracy for CNN: 0.9895\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Aluguel       1.00      1.00      1.00        53\n",
            "        Imposto       1.00      1.00      1.00       167\n",
            " Operacao_Bolsa       0.97      1.00      0.99        71\n",
            "Operacao_Outras       1.00      0.97      0.98        61\n",
            "    Operacao_RF       1.00      0.92      0.96        25\n",
            "Rendimento_Acao       0.97      1.00      0.99       115\n",
            " Rendimento_FII       0.99      0.99      0.99        86\n",
            "  Rendimento_RF       1.00      0.80      0.89        10\n",
            "     Resgate_RF       1.00      1.00      1.00        31\n",
            "           Taxa       0.98      1.00      0.99        50\n",
            "\n",
            "       accuracy                           0.99       669\n",
            "      macro avg       0.99      0.97      0.98       669\n",
            "   weighted avg       0.99      0.99      0.99       669\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos Neurais - Predições"
      ],
      "metadata": {
        "id": "5h-zIs7QW4S6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUzEiQocYNvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNaVwhrJl8HjN6vBmYZu8O0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}